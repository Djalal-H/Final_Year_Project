{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Attention Extraction\n",
    "\n",
    "This notebook loads a trained model and extracts attention weights from the Wayformer encoder\n",
    "by running inference on scenarios from the dataset.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load trained model checkpoint\n",
    "2. Load scenarios from dataset\n",
    "3. Run forward pass with `return_attention_weights=True`\n",
    "4. Save/analyze the extracted attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import yaml\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add project root to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 01:49:35.778243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769046575.792225   59316 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769046575.796661   59316 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/home/djalal/conda-envs/vmax/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "from waymax import dynamics\n",
    "from vmax.simulator import make_env_for_evaluation, datasets, make_data_generator\n",
    "from vmax.agents.learning.reinforcement.ppo import ppo_factory\n",
    "from vmax.scripts.evaluate.utils import load_params, load_yaml_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"✓ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DIR = \"../../runs/PPO_VEC_WAYFORMER/\"  # Path to training run\n",
    "DATASET_PATH = \"../../training.tfrecord\"  # Dataset name or path\n",
    "NUM_SCENARIOS = 10  # Number of scenarios to process\n",
    "OUTPUT_DIR = \"attention_extractions/\"  # Where to save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join(RUN_DIR, \"model\")\n",
    "CONFIG_PATH = os.path.join(RUN_DIR, \".hydra/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory: ../../runs/PPO_VEC_WAYFORMER/\n",
      "Config path: ../../runs/PPO_VEC_WAYFORMER/.hydra/config.yaml\n"
     ]
    }
   ],
   "source": [
    "print(f\"Run directory: {RUN_DIR}\")\n",
    "print(f\"Config path: {CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ENCODER (Wayformer) LOCALLY\n",
    "TO ENSURE SUPPORT FOR ATTENTION EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REDEFINE ENCODER LOCALLY TO ENSURE SUPPORT FOR ATTENTION EXTRACTION ---\n",
    "# It appears the installed version of WayformerEncoder might lack the \n",
    "# return_attention_weights feature. We define it here to be safe.\n",
    "\n",
    "import einops\n",
    "import jax.nn.initializers as init\n",
    "from flax import linen as nn\n",
    "from vmax.agents.networks import encoders\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from waymax import datatypes\n",
    "\n",
    "class LocalWayformerAttention(nn.Module):\n",
    "    depth: int = 2\n",
    "    num_latents: int = 32\n",
    "    num_heads: int = 2\n",
    "    head_features: int = 16\n",
    "    ff_mult: int = 1\n",
    "    attn_dropout: float = 0.0\n",
    "    ff_dropout: float = 0.0\n",
    "    return_attention_weights: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None):\n",
    "        bs, dim = x.shape[0], x.shape[-1]\n",
    "        latents = self.param(\"latents\", init.normal(), (self.num_latents, dim * self.ff_mult))\n",
    "        latent = einops.repeat(latents, \"n d -> b n d\", b=bs)\n",
    "        x = einops.rearrange(x, \"b n ... -> b n (...)\")\n",
    "\n",
    "        attention_weights = {} if self.return_attention_weights else None\n",
    "\n",
    "        # FIX: Pass return_attention_weights during class instantiation, not during call\n",
    "        attn = partial(\n",
    "            encoders.AttentionLayer,\n",
    "            heads=self.num_heads,\n",
    "            head_features=self.head_features,\n",
    "            dropout=self.attn_dropout,\n",
    "            return_attention_weights=self.return_attention_weights  # Pass here\n",
    "        )\n",
    "        ff = partial(encoders.FeedForward, mult=self.ff_mult, dropout=self.ff_dropout)\n",
    "        \n",
    "        # Cross-attention (attn_0)\n",
    "        rz = encoders.ReZero(name=\"rezero_0\")\n",
    "        if self.return_attention_weights:\n",
    "            attn_out, attn_w = attn(name=\"attn_0\")(latent, x, mask_k=mask)  # Don't pass return_attention_weights here\n",
    "            latent += rz(attn_out)\n",
    "            attention_weights['cross_attn_0'] = attn_w\n",
    "        else:\n",
    "            latent += rz(attn(name=\"attn_0\")(latent, x, mask_k=mask))\n",
    "        latent += rz(ff(name=\"ff_0\")(latent))\n",
    "\n",
    "        # Self-attention layers\n",
    "        for i in range(1, self.depth):\n",
    "            rz = encoders.ReZero(name=f\"rezero_{i}\")\n",
    "            if self.return_attention_weights:\n",
    "                attn_out, attn_w = attn(name=f\"attn_{i}\")(latent)  # Don't pass return_attention_weights here\n",
    "                latent += rz(attn_out)\n",
    "                attention_weights[f'self_attn_{i}'] = attn_w\n",
    "            else:\n",
    "                latent += rz(attn(name=f\"attn_{i}\")(latent))\n",
    "            latent += rz(ff(name=f\"ff_{i}\")(latent))\n",
    "\n",
    "        if self.return_attention_weights:\n",
    "            return latent, attention_weights\n",
    "        return latent\n",
    "\n",
    "class LocalWayformerEncoder(nn.Module):\n",
    "    unflatten_fn: callable = lambda x: x\n",
    "    embedding_layer_sizes: tuple = (256, 256)\n",
    "    embedding_activation: datatypes.ActivationFn = nn.relu\n",
    "    attention_depth: int = 2\n",
    "    dk: int = 64\n",
    "    num_latents: int = 64\n",
    "    latent_num_heads: int = 4\n",
    "    latent_head_features: int = 64\n",
    "    ff_mult: int = 2\n",
    "    attn_dropout: float = 0.0\n",
    "    ff_dropout: float = 0.0\n",
    "    fusion_type: str = \"late\"\n",
    "    return_attention_weights: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, obs: jax.Array):\n",
    "        # Add batch dimension if missing\n",
    "        if obs.ndim == 1:\n",
    "            obs = jnp.expand_dims(obs, axis=0)\n",
    "        \n",
    "        features, masks = self.unflatten_fn(obs)\n",
    "        sdc_traj_features, other_traj_features, rg_features, tl_features, gps_path_features = features\n",
    "        sdc_traj_valid_mask, other_traj_valid_mask, rg_valid_mask, tl_valid_mask = masks\n",
    "\n",
    "        # Ensure all features have batch dimension\n",
    "        def ensure_batch_dim(x):\n",
    "            if x.ndim == 2:\n",
    "                return jnp.expand_dims(x, axis=0)\n",
    "            return x\n",
    "        \n",
    "        sdc_traj_features = ensure_batch_dim(sdc_traj_features)\n",
    "        other_traj_features = ensure_batch_dim(other_traj_features)\n",
    "        rg_features = ensure_batch_dim(rg_features)\n",
    "        tl_features = ensure_batch_dim(tl_features)\n",
    "        gps_path_features = ensure_batch_dim(gps_path_features)\n",
    "        \n",
    "        sdc_traj_valid_mask = ensure_batch_dim(sdc_traj_valid_mask)\n",
    "        other_traj_valid_mask = ensure_batch_dim(other_traj_valid_mask)\n",
    "        rg_valid_mask = ensure_batch_dim(rg_valid_mask)\n",
    "        tl_valid_mask = ensure_batch_dim(tl_valid_mask)\n",
    "\n",
    "        num_objects, timestep_agent = other_traj_features.shape[-3:-1]\n",
    "        num_roadgraph = rg_features.shape[-2]\n",
    "        target_len = gps_path_features.shape[-2]\n",
    "        num_light, timestep_tl = tl_features.shape[-3:-1]\n",
    "\n",
    "        # Embeddings\n",
    "        sdc_traj_encoding = encoders.build_mlp_embedding(sdc_traj_features, self.dk, self.embedding_layer_sizes, self.embedding_activation, \"sdc_traj_enc\")\n",
    "        other_traj_encoding = encoders.build_mlp_embedding(other_traj_features, self.dk, self.embedding_layer_sizes, self.embedding_activation, \"other_traj_enc\")\n",
    "        rg_encoding = encoders.build_mlp_embedding(rg_features, self.dk, self.embedding_layer_sizes, self.embedding_activation, \"rg_enc\")\n",
    "        tl_encoding = encoders.build_mlp_embedding(tl_features, self.dk, self.embedding_layer_sizes, self.embedding_activation, \"tl_enc\")\n",
    "        gps_path_encoding = encoders.build_mlp_embedding(gps_path_features, self.dk, self.embedding_layer_sizes, self.embedding_activation, \"gps_path_enc\")\n",
    "\n",
    "        # PE and Temporal Encoding setup (Simplified)\n",
    "        sdc_traj_encoding += jnp.expand_dims(self.param(\"sdc_traj_pe\", init.normal(), (1, timestep_agent, self.dk)), 0)\n",
    "        other_traj_encoding += jnp.expand_dims(self.param(\"other_traj_pe\", init.normal(), (num_objects, timestep_agent, self.dk)), 0)\n",
    "        rg_encoding += jnp.expand_dims(self.param(\"rg_pe\", init.normal(), (num_roadgraph, self.dk)), 0)\n",
    "        tl_encoding += jnp.expand_dims(self.param(\"tj_pe\", init.normal(), (num_light, timestep_tl, self.dk)), 0)\n",
    "        gps_path_encoding += jnp.expand_dims(self.param(\"gps_path_pe\", init.normal(), (target_len, self.dk)), 0)\n",
    "\n",
    "        temp_pe_agents = self.param(\"temp_pe_agents\", init.normal(), (timestep_agent,))\n",
    "        temp_pe_tl = self.param(\"temp_pe_tl\", init.normal(), (timestep_tl,))\n",
    "        sdc_traj_encoding += temp_pe_agents[None, None, :, None]\n",
    "        other_traj_encoding += temp_pe_agents[None, None, :, None]\n",
    "        tl_encoding += temp_pe_tl[None, None, :, None]\n",
    "\n",
    "        # Reshaping\n",
    "        sdc_traj_encoding = einops.rearrange(sdc_traj_encoding, \"b n t d -> b (n t) d\")\n",
    "        other_traj_encoding = einops.rearrange(other_traj_encoding, \"b n t d -> b (n t) d\")\n",
    "        tl_encoding = einops.rearrange(tl_encoding, \"b n t d -> b (n t) d\")\n",
    "        \n",
    "        sdc_traj_valid_mask = einops.rearrange(sdc_traj_valid_mask, \"b n t -> b (n t)\")\n",
    "        other_traj_valid_mask = einops.rearrange(other_traj_valid_mask, \"b n t -> b (n t)\")\n",
    "        tl_valid_mask = einops.rearrange(tl_valid_mask, \"b n t -> b (n t)\")\n",
    "        \n",
    "        all_attention_weights = {} if self.return_attention_weights else None\n",
    "\n",
    "        self_attn = partial(\n",
    "            LocalWayformerAttention,\n",
    "            num_latents=self.num_latents,\n",
    "            num_heads=self.latent_num_heads,\n",
    "            head_features=self.latent_head_features,\n",
    "            ff_mult=self.ff_mult,\n",
    "            attn_dropout=self.attn_dropout,\n",
    "            ff_dropout=self.ff_dropout,\n",
    "            return_attention_weights=self.return_attention_weights  # Pass here\n",
    "        )\n",
    "\n",
    "        def call_attn(attn_module, embeddings, mask, prefix):\n",
    "            if self.return_attention_weights:\n",
    "                out, attn_w = attn_module()(embeddings, mask)  # Don't pass return_attention_weights here\n",
    "                for k, v in attn_w.items():\n",
    "                    all_attention_weights[f'{prefix}/{k}'] = v\n",
    "                return out\n",
    "            else:\n",
    "                return attn_module()(embeddings, mask)\n",
    "\n",
    "        # Late fusion (default)\n",
    "        output_sdc_traj = call_attn(partial(self_attn, depth=self.attention_depth, name=\"sdc_traj_attention\"), sdc_traj_encoding, sdc_traj_valid_mask, \"sdc_traj\")\n",
    "        output_other_traj = call_attn(partial(self_attn, depth=self.attention_depth, name=\"other_traj_attention\"), other_traj_encoding, other_traj_valid_mask, \"other_traj\")\n",
    "        output_rg = call_attn(partial(self_attn, depth=self.attention_depth, name=\"rg_attention\"), rg_encoding, rg_valid_mask, \"roadgraph\")\n",
    "        output_tl = call_attn(partial(self_attn, depth=self.attention_depth, name=\"tl_attention\"), tl_encoding, tl_valid_mask, \"traffic_lights\")\n",
    "        output_gps_path = call_attn(partial(self_attn, depth=self.attention_depth, name=\"gps_path_attention\"), gps_path_encoding, None, \"gps_path\")\n",
    "\n",
    "        output = jnp.concatenate([output_sdc_traj, output_other_traj, output_rg, output_tl, output_gps_path], axis=-2)\n",
    "        output = output.mean(axis=1)\n",
    "\n",
    "        if self.return_attention_weights:\n",
    "            return output, all_attention_weights\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup the environment and load the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_env(config_path, model_dir, model_name=\"model_final.pkl\"):\n",
    "    \"Load trained model and create matching environment.\"\n",
    "    config = load_yaml_config(config_path)\n",
    "\n",
    "    # Flatten config\n",
    "    if \"algorithm\" in config and \"network\" in config[\"algorithm\"]:\n",
    "        config[\"policy\"] = config[\"algorithm\"][\"network\"].get(\"policy\", {})\n",
    "        config[\"value\"] = config[\"algorithm\"][\"network\"].get(\"value\", {})\n",
    "        config[\"action_distribution\"] = config[\"algorithm\"][\"network\"].get(\"action_distribution\", \"gaussian\")\n",
    "    if \"network\" in config and \"encoder\" in config[\"network\"]:\n",
    "        config[\"encoder\"] = config[\"network\"][\"encoder\"]\n",
    "\n",
    "    # FORCE 'vec' type to ensure flattened observations\n",
    "    obs_type = config.get(\"observation_type\", \"vec\")\n",
    "    if obs_type in [\"idm\", \"pdm\"]:\n",
    "        print(f\"Warning: Configured observation_type '{obs_type}' does not support WayformerEncoder attention extraction.\")\n",
    "        print(f\"Forcing observation_type='vec' to ensure flattened observations.\")\n",
    "        obs_type = \"vec\"\n",
    "\n",
    "    # Create Env\n",
    "    term_keys = config.get(\"termination_keys\", [\"offroad\", \"overlap\"])\n",
    "    env = make_env_for_evaluation(\n",
    "        max_num_objects=config.get(\"max_num_objects\", 64),\n",
    "        dynamics_model=dynamics.InvertibleBicycleModel(normalize_actions=True),\n",
    "        sdc_paths_from_data=not config.get(\"waymo_dataset\", False),\n",
    "        observation_type=obs_type,\n",
    "        observation_config=config.get(\"observation_config\", {}),\n",
    "        termination_keys=term_keys,\n",
    "    )\n",
    "\n",
    "    # Build PPO Networks\n",
    "    obs_size = env.observation_spec()\n",
    "    action_size = env.action_spec().data.shape[0]\n",
    "    unflatten_fn = env.get_wrapper_attr(\"features_extractor\").unflatten_features\n",
    "    \n",
    "    networks = ppo_factory.make_networks(\n",
    "        observation_size=obs_size,\n",
    "        action_size=action_size,\n",
    "        unflatten_fn=unflatten_fn,\n",
    "        learning_rate=3e-4, \n",
    "        network_config=config\n",
    "    )\n",
    "\n",
    "    # --- USE LOCAL ENCODER ---\n",
    "    from vmax.agents.networks import network_utils\n",
    "    encoder_cfg = network_utils.parse_config(config[\"encoder\"], \"encoder\")\n",
    "    \n",
    "    # FIX: Convert string activations (e.g. 'relu') to callables\n",
    "    encoder_cfg = network_utils.convert_to_dict_with_activation_fn(encoder_cfg)\n",
    "    \n",
    "    # Manually instantiate our LocalWayformerEncoder with return_attention_weights=True\n",
    "    encoder = LocalWayformerEncoder(unflatten_fn, return_attention_weights=True, **encoder_cfg)\n",
    "\n",
    "    # Load parameters\n",
    "    model_files = sorted(glob.glob(os.path.join(model_dir, \"*.pkl\")))\n",
    "    if \"model_final.pkl\" in [os.path.basename(f) for f in model_files]:\n",
    "        model_path = os.path.join(model_dir, \"model_final.pkl\")\n",
    "    else:\n",
    "        model_path = model_files[-1] if model_files else None\n",
    "    \n",
    "    if not model_path:\n",
    "        raise FileNotFoundError(f\"No model found in {model_dir}\")\n",
    "    \n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    params = load_params(model_path)\n",
    "    \n",
    "    return env, networks, params, config, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: ../../runs/PPO_VEC_WAYFORMER/model/model_final.pkl\n",
      "✓ Model and environment loaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(CONFIG_PATH):\n",
    "    env, networks, params, config, encoder = setup_model_and_env(CONFIG_PATH, MODEL_DIR)\n",
    "    print(\"✓ Model and environment loaded\")\n",
    "else:\n",
    "    print(f\"⚠ Config not found at {CONFIG_PATH}\")\n",
    "    print(\"  Please update RUN_DIR to point to your training run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalWayformerEncoder(\n",
      "    # attributes\n",
      "    unflatten_fn = unflatten_features\n",
      "    embedding_layer_sizes = [256, 256]\n",
      "    embedding_activation = 'relu'\n",
      "    attention_depth = 2\n",
      "    dk = 64\n",
      "    num_latents = 16\n",
      "    latent_num_heads = 2\n",
      "    latent_head_features = 16\n",
      "    ff_mult = 2\n",
      "    attn_dropout = 0.0\n",
      "    ff_dropout = 0.0\n",
      "    fusion_type = 'late'\n",
      "    return_attention_weights = True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_from_scenario(env, networks, params, encoder, scenario, rng_key):\n",
    "    \"Run a single scenario through the model and extract attention weights.\"\n",
    "    \n",
    "    # Observe the scenario (faster than full reset)\n",
    "    obs = env.observe(scenario)\n",
    "\n",
    "    if isinstance(obs, tuple):\n",
    "        raise ValueError(f\"Observation is a tuple, expected flattened array. Ensure observation_type='vec'.\")\n",
    "    \n",
    "    # Extract encoder parameters\n",
    "    if 'params' in params.policy and 'encoder_layer' in params.policy['params']:\n",
    "        encoder_params = params.policy['params']['encoder_layer']\n",
    "    else:\n",
    "        print(\"Warning: Could not find 'encoder_layer' in params. Using policy params root.\")\n",
    "        encoder_params = params.policy\n",
    "    \n",
    "    @jax.jit\n",
    "    def forward_with_attention(e_params, observation):\n",
    "        \"Forward pass that returns attention weights.\"\n",
    "        \n",
    "        latent, attention_weights = encoder.apply(\n",
    "            {'params': e_params},\n",
    "            observation\n",
    "        )\n",
    "        return latent, attention_weights\n",
    "    \n",
    "    try:\n",
    "        latent, attn_weights = forward_with_attention(encoder_params, obs)\n",
    "        \n",
    "        return {\n",
    "            'observation': jax.device_get(obs),\n",
    "            'latent': jax.device_get(latent),\n",
    "            'attention_weights': jax.tree_map(lambda x: np.array(jax.device_get(x)), attn_weights),\n",
    "            'success': True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during forward pass: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {'success': False, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction(env, networks, params, config, dataset_path, num_scenarios, output_dir):\n",
    "    \"\"\"Process multiple scenarios and save attention weights.\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create data generator\n",
    "    data_gen = make_data_generator(\n",
    "        path=datasets.get_dataset(dataset_path),\n",
    "        max_num_objects=config[\"max_num_objects\"],\n",
    "        include_sdc_paths=not config.get(\"waymo_dataset\", False),\n",
    "        batch_dims=(1,),  # Single scenario at a time\n",
    "        seed=42,\n",
    "        repeat=1,\n",
    "    )\n",
    "    \n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {num_scenarios} scenarios...\")\n",
    "    \n",
    "    for i, scenario in enumerate(data_gen):\n",
    "        if i >= num_scenarios:\n",
    "            break\n",
    "            \n",
    "        print(f\"  Scenario {i+1}/{num_scenarios}\", end=\"\")\n",
    "        \n",
    "        # Squeeze batch dim\n",
    "        scenario = jax.tree_map(lambda x: x.squeeze(0), scenario)\n",
    "        \n",
    "        rng_key, extract_key = jax.random.split(rng_key)\n",
    "        result = extract_attention_from_scenario(env, networks, params, encoder, scenario, extract_key)\n",
    "        \n",
    "        if result['success']:\n",
    "            # Save individual result\n",
    "            save_path = os.path.join(output_dir, f\"attention_scenario_{i:04d}.pkl\")\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(result, f)\n",
    "            print(f\" ✓ Saved to {save_path}\")\n",
    "            results.append(result)\n",
    "        else:\n",
    "            print(f\" ✗ Failed\")\n",
    "    \n",
    "    print(f\"\\n✓ Extracted attention from {len(results)}/{num_scenarios} scenarios\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 scenarios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1769046621.925625   59316 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3602 MB memory:  -> device: 0, name: NVIDIA H100 NVL MIG 1g.24gb, pci bus id: 0000:45:00.0, compute capability: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scenario 1/10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59316/1720437872.py:28: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  scenario = jax.tree_map(lambda x: x.squeeze(0), scenario)\n",
      "E0122 01:50:33.910496   59316 hlo_lexer.cc:443] Failed to parse int literal: 57906653232081209837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Scenario 2/10  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Scenario 4/10  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Scenario 6/10  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Scenario 8/10  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "  Scenario 10/10  Error during forward pass: 'tuple' object has no attribute 'shape'\n",
      " ✗ Failed\n",
      "\n",
      "✓ Extracted attention from 0/10 scenarios\n"
     ]
    }
   ],
   "source": [
    "if 'env' in dir():\n",
    "    results = run_extraction(env, networks, params, config, DATASET_PATH, NUM_SCENARIOS, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention(results):\n",
    "    \"\"\"Basic analysis of extracted attention weights.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Get attention keys from first result\n",
    "    sample = results[0]['attention_weights']\n",
    "    print(\"Attention weight keys:\")\n",
    "    for key in sample.keys():\n",
    "        shape = sample[key].shape\n",
    "        print(f\"  {key}: {shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Attention Statistics (averaged over scenarios)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for key in sample.keys():\n",
    "        weights = [r['attention_weights'][key] for r in results]\n",
    "        stacked = np.stack(weights)\n",
    "        \n",
    "        print(f\"\\n{key}:\")\n",
    "        print(f\"  Shape: {stacked.shape}\")\n",
    "        print(f\"  Mean:  {stacked.mean():.4f}\")\n",
    "        print(f\"  Std:   {stacked.std():.4f}\")\n",
    "        print(f\"  Max:   {stacked.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results' in dir() and results:\n",
    "    analyze_attention(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Attention (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(attention_weights, key, head_idx=0, save_path=None):\n",
    "    \"\"\"Plot attention heatmap for a specific layer and head.\"\"\"\n",
    "    \n",
    "    if key not in attention_weights:\n",
    "        print(f\"Key '{key}' not found. Available: {list(attention_weights.keys())}\")\n",
    "        return\n",
    "    \n",
    "    attn = attention_weights[key]\n",
    "    \n",
    "    # Shape is typically (batch, heads, query, key) or (heads, query, key)\n",
    "    if attn.ndim == 4:\n",
    "        attn = attn[0]  # Take first batch\n",
    "    \n",
    "    if attn.ndim == 3:\n",
    "        attn = attn[head_idx]  # Take specific head\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attn, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.title(f'Attention: {key} (Head {head_idx})')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Plot first result's attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results' in dir() and results:\n",
    "    sample_attn = results[0]['attention_weights']\n",
    "    first_key = list(sample_attn.keys())[0]\n",
    "    plot_attention_heatmap(sample_attn, first_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VMax Environment",
   "language": "python",
   "name": "vmax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
